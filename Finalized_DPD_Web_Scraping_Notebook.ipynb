{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEdWnOm1b6jimkoS3FgBc+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Peter-Hou/DPD-Project/blob/main/Finalized_DPD_Web_Scraping_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "av8uBShshXZz"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import bs4\n",
        "import io\n",
        "\n",
        "def download_zip_files():\n",
        "    file_urls = {\n",
        "        'allfiles.zip': \"https://www.canada.ca/content/dam/hc-sc/documents/services/drug-product-database/allfiles.zip\",\n",
        "        'allfiles_ia.zip': \"https://www.canada.ca/content/dam/hc-sc/documents/services/drug-product-database/allfiles_ia.zip\",\n",
        "        'allfiles_ap.zip': \"https://www.canada.ca/content/dam/hc-sc/documents/services/drug-product-database/allfiles_ap.zip\",\n",
        "        'allfiles_dr.zip': \"https://www.canada.ca/content/dam/hc-sc/documents/services/drug-product-database/allfiles_dr.zip\"\n",
        "    }\n",
        "\n",
        "    zip_files = {}\n",
        "    for file_name, url in file_urls.items():\n",
        "        # Send a request to download the file content\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # Read the file content as a BytesIO object\n",
        "        zip_file = io.BytesIO(response.content)\n",
        "\n",
        "        # store each downloaded zip file's content as an item in a dictionary\n",
        "        zip_files[file_name] = zip_file\n",
        "    return zip_files\n",
        "\n",
        "zip_files = download_zip_files()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "# Note each zip file contains multiple data files\n",
        "# Get the names of each file contained in each zip file\n",
        "with zipfile.ZipFile(zip_files['allfiles.zip'], 'r') as allfiles_zip:\n",
        "    af_names = allfiles_zip.namelist()\n",
        "\n",
        "with zipfile.ZipFile(zip_files['allfiles_ia.zip'], 'r') as allfiles_ia_zip:\n",
        "    af_ia_names = allfiles_ia_zip.namelist()\n",
        "\n",
        "with zipfile.ZipFile(zip_files['allfiles_ap.zip'], 'r') as allfiles_ap_zip:\n",
        "    af_ap_names = allfiles_ap_zip.namelist()\n",
        "\n",
        "with zipfile.ZipFile(zip_files['allfiles_dr.zip'], 'r') as allfiles_dr_zip:\n",
        "    af_dr_names = allfiles_dr_zip.namelist()\n",
        "\n",
        "all_file_names = []\n",
        "all_file_names.extend(af_names)\n",
        "all_file_names.extend(af_ia_names)\n",
        "all_file_names.extend(af_ap_names)\n",
        "all_file_names.extend(af_dr_names)\n"
      ],
      "metadata": {
        "id": "aN6tqzYdhmTT"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import io\n",
        "\n",
        "# all files contained in the zip file do not have column names, only raw data\n",
        "# column names are listed in a separate website\n",
        "\n",
        "# Get URL for the page containing column names\n",
        "url = \"https://www.canada.ca/en/health-canada/services/drugs-health-products/drug-products/drug-product-database/read-file-drug-product-database-data-extract.html\"\n",
        "\n",
        "# Make a request to the webpage and get the HTML content\n",
        "html_content = requests.get(url).text\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Find all the tables in the webpage\n",
        "tables = soup.find_all('table')\n",
        "\n",
        "\n",
        "# Initialize an empty dictionary to store the column names\n",
        "column_names_dict = {}\n",
        "column_names = []\n",
        "\n",
        "# Loop through each table in the webpage\n",
        "for table in tables:\n",
        "\n",
        "    # Find the title of the table (i.e., the file name)\n",
        "    table_title = table.find('strong').text.strip()\n",
        "\n",
        "    # Find all the rows in the table\n",
        "    rows = table.find_all('tr')\n",
        "\n",
        "    # Extract the column names from the first row of the table\n",
        "    for row in rows:\n",
        "        td = row.find('td')\n",
        "        column_names.append(td.text.strip(\"*\")) if td else None\n",
        "\n",
        "    # Store the column names in the dictionary\n",
        "    column_names_dict[table_title] = column_names\n",
        "    column_names = []\n",
        "\n",
        "\n",
        "column_names_dict['QRYM_BIOSIMILAR'] = ['DRUG_CODE', 'TYPE', 'TYPE_F', 'CLASS_CODE']\n",
        "\n",
        "def remove_filename_suffix(filename):\n",
        "    if '_' in filename:\n",
        "        filename = filename[:filename.find('_')]\n",
        "    else:\n",
        "        filename = filename[:filename.find('.')]\n",
        "    return filename\n",
        "\n",
        "def remove_formal_filename_prefix(formal_filename):\n",
        "    start_ind = formal_filename.find(\"_\")\n",
        "    return formal_filename[start_ind + 1:].lower()\n",
        "\n",
        "def create_name_mapping(file_names, formal_file_names):\n",
        "    name_mapping = {}\n",
        "    for filename in file_names:\n",
        "        original_filename = filename\n",
        "        filename = remove_filename_suffix(filename)\n",
        "        not_match = True\n",
        "        while not_match:\n",
        "            for formal_filename in formal_file_names:\n",
        "                #print(formal_filename)\n",
        "                #print(filename)\n",
        "                original_formal_filename = formal_filename\n",
        "                formal_filename = remove_formal_filename_prefix(formal_filename)\n",
        "                ind = formal_filename.find(\"_\") if \"_\" in formal_filename else len(formal_filename)\n",
        "                if filename == formal_filename[:len(filename)] or \\\n",
        "                    filename == formal_filename[ind + 1: ind + len(filename) + 1]:\n",
        "                    #print(original_filename)\n",
        "                    #print(original_formal_filename)\n",
        "                    name_mapping[original_filename] = original_formal_filename\n",
        "                    not_match = False\n",
        "                    break\n",
        "            if not not_match:\n",
        "                break\n",
        "            if len(filename) - 1 != 0:\n",
        "                filename = filename[:len(filename) - 1]\n",
        "            #else:\n",
        "             #   raise ValueError(f'Did not find a match column name for {original_filename} when concatenating the column names to data extracts')\n",
        "    return name_mapping\n",
        "\n",
        "# inactive.txt is breaking the consistent order between all the zip files\n",
        "# it's information has been included in the other files of allfiles_ia.zip\n",
        "af_ia_names.remove('inactive.txt')\n",
        "name_mapping = create_name_mapping(af_names, column_names_dict.keys())\n",
        "name_mapping1 = create_name_mapping(af_ia_names, column_names_dict.keys())\n",
        "name_mapping2 = create_name_mapping(af_ap_names, column_names_dict.keys())\n",
        "name_mapping3 = create_name_mapping(af_dr_names, column_names_dict.keys())\n",
        "\n",
        "name_mapping.update(name_mapping1)\n",
        "name_mapping.update(name_mapping2)\n",
        "name_mapping.update(name_mapping3)\n",
        "\n",
        "print(len(name_mapping)) ## it is supposed to be 12 * 4 = 48\n",
        "print(name_mapping)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEub-sRmjMtO",
        "outputId": "971882bf-127c-497c-ed4e-0667e2e114dc"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48\n",
            "{'biosimilar.txt': 'QRYM_BIOSIMILAR', 'comp.txt': 'QRYM_COMPANIES', 'drug.txt': 'QRYM_DRUG_PRODUCT', 'form.txt': 'QRYM_FORM', 'ingred.txt': 'QRYM_ACTIVE_INGREDIENTS', 'package.txt': 'QRYM_PACKAGING', 'pharm.txt': 'QRYM_PHARMACEUTICAL_STD', 'route.txt': 'QRYM_ROUTE', 'schedule.txt': 'QRYM_SCHEDULE', 'status.txt': 'QRYM_STATUS', 'ther.txt': 'QRYM_THERAPEUTIC_CLASS', 'vet.txt': 'QRYM_VETERINARY_SPECIES', 'biosimilar_ia.txt': 'QRYM_BIOSIMILAR', 'comp_ia.txt': 'QRYM_COMPANIES', 'drug_ia.txt': 'QRYM_DRUG_PRODUCT', 'form_ia.txt': 'QRYM_FORM', 'ingred_ia.txt': 'QRYM_ACTIVE_INGREDIENTS', 'package_ia.txt': 'QRYM_PACKAGING', 'pharm_ia.txt': 'QRYM_PHARMACEUTICAL_STD', 'route_ia.txt': 'QRYM_ROUTE', 'schedule_ia.txt': 'QRYM_SCHEDULE', 'status_ia.txt': 'QRYM_STATUS', 'ther_ia.txt': 'QRYM_THERAPEUTIC_CLASS', 'vet_ia.txt': 'QRYM_VETERINARY_SPECIES', 'biosimilar_ap.txt': 'QRYM_BIOSIMILAR', 'comp_ap.txt': 'QRYM_COMPANIES', 'drug_ap.txt': 'QRYM_DRUG_PRODUCT', 'form_ap.txt': 'QRYM_FORM', 'ingred_ap.txt': 'QRYM_ACTIVE_INGREDIENTS', 'package_ap.txt': 'QRYM_PACKAGING', 'pharm_ap.txt': 'QRYM_PHARMACEUTICAL_STD', 'route_ap.txt': 'QRYM_ROUTE', 'schedule_ap.txt': 'QRYM_SCHEDULE', 'status_ap.txt': 'QRYM_STATUS', 'ther_ap.txt': 'QRYM_THERAPEUTIC_CLASS', 'vet_ap.txt': 'QRYM_VETERINARY_SPECIES', 'biosimilar_dr.txt': 'QRYM_BIOSIMILAR', 'comp_dr.txt': 'QRYM_COMPANIES', 'drug_dr.txt': 'QRYM_DRUG_PRODUCT', 'form_dr.txt': 'QRYM_FORM', 'ingred_dr.txt': 'QRYM_ACTIVE_INGREDIENTS', 'package_dr.txt': 'QRYM_PACKAGING', 'pharm_dr.txt': 'QRYM_PHARMACEUTICAL_STD', 'route_dr.txt': 'QRYM_ROUTE', 'schedule_dr.txt': 'QRYM_SCHEDULE', 'status_dr.txt': 'QRYM_STATUS', 'ther_dr.txt': 'QRYM_THERAPEUTIC_CLASS', 'vet_dr.txt': 'QRYM_VETERINARY_SPECIES'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "for zip_file in zip_files.values():\n",
        "\n",
        "    with zipfile.ZipFile(zip_file, 'r') as files:\n",
        "        file_names = files.namelist()\n",
        "\n",
        "    for file_name in file_names:\n",
        "        with zipfile.ZipFile(zip_file, 'r') as files:\n",
        "            with files.open(file_name) as file:\n",
        "                # Find the matched column names\n",
        "                if file_name == 'inactive.txt':\n",
        "                  # info contains in this file have been included in all other files\n",
        "                    continue\n",
        "                formal_file_name = name_mapping[file_name]\n",
        "                column_names = column_names_dict[formal_file_name]\n",
        "\n",
        "                df = pd.read_csv(file, sep=',', header=None, names=column_names, encoding = 'utf-8')\n",
        "                globals()[file_name.split('.')[0]] = df\n"
      ],
      "metadata": {
        "id": "eyzTayLtrUXH"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drug_code is the joining key shared by each file\n",
        "# join all files together of their respective din status\n",
        "\n",
        "merged_active = drug.merge(biosimilar, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(comp, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(form, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(ingred, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(package, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(pharm, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(route, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(schedule, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(status, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(ther, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(vet, on='DRUG_CODE', how='left')\n",
        "\n",
        "merged_inactive = drug_ia.merge(biosimilar_ia, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(comp_ia, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(form_ia, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(ingred_ia, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(package_ia, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(pharm_ia, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(route_ia, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(schedule_ia, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(status_ia, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(ther_ia, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(vet_ia, on='DRUG_CODE', how='left')\n",
        "\n",
        "merged_dormant = drug_dr.merge(biosimilar_dr, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(comp_dr, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(form_dr, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(ingred_dr, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(package_dr, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(pharm_dr, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(route_dr, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(schedule_dr, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(status_dr, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(ther_dr, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(vet_dr, on='DRUG_CODE', how='left')\n",
        "\n",
        "merged_approved = drug_ap.merge(biosimilar_ap, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(comp_ap, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(form_ap, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(ingred_ap, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(package_ap, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(pharm_ap, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(route_ap, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(schedule_ap, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(status_ap, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(ther_ap, on='DRUG_CODE', how='left') \\\n",
        "                  .merge(vet_ap, on='DRUG_CODE', how='left')\n"
      ],
      "metadata": {
        "id": "05d1ZD-RtwXe"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Extracts all ingredient codes that are biosimilar type from all files\n",
        "active_biosimilar_ingred_codes = merged_active['ACTIVE_INGREDIENT_CODE'][merged_active['TYPE'] == 'Biosimilar'].unique()\n",
        "inactive_biosimilar_ingred_codes = merged_inactive['ACTIVE_INGREDIENT_CODE'][merged_inactive['TYPE'] == 'Biosimilar'].unique()\n",
        "dormant_biosimilar_ingred_codes = merged_dormant['ACTIVE_INGREDIENT_CODE'][merged_dormant['TYPE'] == 'Biosimilar'].unique()\n",
        "approved_biosimilar_ingred_codes = merged_approved['ACTIVE_INGREDIENT_CODE'][merged_approved['TYPE'] == 'Biosimilar'].unique()\n",
        "\n",
        "# Put all ingredient codes that are biosimilar type into one array\n",
        "biosimilar_ingred_codes = np.concatenate((active_biosimilar_ingred_codes, inactive_biosimilar_ingred_codes, dormant_biosimilar_ingred_codes, approved_biosimilar_ingred_codes), axis = None)\n",
        "\n",
        "# Replace all entries in each file which the ingredient code is in biosimilar ingredient code, the class is Human, and the type is NA with biologic type\n",
        "active_mask =  (merged_active['CLASS'] == 'Human') & (merged_active['ACTIVE_INGREDIENT_CODE'].isin(biosimilar_ingred_codes)) & (merged_active['TYPE'].isna())\n",
        "merged_active.loc[active_mask, \"TYPE\"] = 'Biologic'\n",
        "merged_active.loc[active_mask, \"TYPE_F\"] = 'Biologique'\n",
        "\n",
        "inactive_mask = (merged_inactive['CLASS'] == 'Human') & (merged_inactive['ACTIVE_INGREDIENT_CODE'].isin(biosimilar_ingred_codes)) & (merged_inactive['TYPE'].isna())\n",
        "merged_inactive.loc[inactive_mask, 'TYPE'] = 'Biologic'\n",
        "merged_inactive.loc[inactive_mask, \"TYPE_F\"] = 'Biologique'\n",
        "\n",
        "dormant_mask = (merged_dormant['CLASS'] == 'Human') & (merged_dormant['ACTIVE_INGREDIENT_CODE'].isin(biosimilar_ingred_codes)) & (merged_dormant['TYPE'].isna())\n",
        "merged_dormant.loc[dormant_mask, 'TYPE'] = 'Biologic'\n",
        "merged_dormant.loc[dormant_mask, \"TYPE_F\"] = 'Biologique'\n",
        "\n",
        "approved_mask = (merged_approved['CLASS'] == 'Human') & (merged_approved['ACTIVE_INGREDIENT_CODE'].isin(biosimilar_ingred_codes)) & (merged_approved['TYPE'].isna())\n",
        "merged_approved.loc[approved_mask, 'TYPE'] = 'Biologic'\n",
        "merged_approved.loc[approved_mask, \"TYPE_F\"] = 'Biologique'\n"
      ],
      "metadata": {
        "id": "mpEDaGaOum7X"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the dataset that has a column with a footnote in its name\n",
        "merged_approved.columns = merged_approved.columns.str.replace('Footnote', '')\n",
        "merged_inactive.columns = merged_inactive.columns.str.replace('Footnote', '')\n",
        "merged_dormant.columns = merged_dormant.columns.str.replace('Footnote', '')\n",
        "merged_active.columns = merged_active.columns.str.replace('Footnote', '')\n",
        "\n",
        "merged_approved.columns = merged_approved.columns.str.replace(' ', '')\n",
        "merged_inactive.columns = merged_inactive.columns.str.replace(' ', '')\n",
        "merged_dormant.columns = merged_dormant.columns.str.replace(' ', '')\n",
        "merged_active.columns = merged_active.columns.str.replace(' ', '')\n",
        "# Combine all datasets into one giant file\n",
        "DIN_MASTER = pd.concat([merged_approved, merged_inactive, merged_dormant, merged_active], ignore_index = True)\n"
      ],
      "metadata": {
        "id": "3A9nRMyPwAtF"
      },
      "execution_count": 62,
      "outputs": []
    }
  ]
}